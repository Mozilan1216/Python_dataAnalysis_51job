一、项目简介
本项目旨在利用爬虫技术从51job网站获取招聘信息，并通过数据可视化技术展示这些数据。通过分析招聘信息的城市薪资对比、学历工资对比、公司规模招聘数量、各省份平均工资以及岗位描述词云等内容，帮助用户更好地了解当前就业市场的情况和趋势。
选题来源：
通过知乎、CSDN、微信公众号、哔哩哔哩等网站搜索相关数据可视化案例，我搜集了一些数据可视化选题及其相关实现的技术栈，经个人兴趣、难度考量以及结合了当前就业问题这一社会热点，我决定对招聘网站的数据进行爬取。


二、数据爬取
1.数据来源分析
（1）明确需求：明确采集的网站以及数据内容
网站：前程无忧
数据：职位信息


（2）抓包分析
有一些网站，数据是储存在专门的数据包链接里和浏览器导航栏的链接不一样。
- 打开开发者工具：F12
- 刷新网页获取数据
- 通过关键字搜索找到对应的数据包地址
- Headers获取数据包链接并查看相应标签信息


2.代码实现步骤——crawl.py
（1）发送请求：模拟浏览器对于url地址发送请求
（2）获取数据：服务器返回相应数据 < 整个返回响应数据 > 
（3）解析数据：提取我们需要的字段
（4）保存数据：报数据保存在表格文件中< csv/Excel>
（5）多页数据采集：更改pagenum即可

3.第二种爬取方法
注：我使用前面的方法完成了招聘信息的爬取，使用下面即将介绍的方法完成了每一条招聘信息的详情页数据的爬取。
（1）获取爬取信息网址
（2）通过driver方法模拟智能打开浏览器并访问网站
（3）通过div标签获取我所需的信息
（4）通过quit退出关闭浏览器，避免资源泄露
另：还可以使用selenium --> 需要下载浏览器对应的驱动，进行操作浏览器，这里我使用的是DrissionPage --> 不需要下载驱动

三种爬虫方法的对比：
（1）Requests
①基于 Python 的 HTTP 库，简单易用。
②适用于静态网页的爬取，对于简单的网页结构和数据请求很方便。
③不支持 JavaScript 渲染，只能获取静态内容，不能获取动态内容。
④不支持模拟用户行为，无法实现复杂的交互操作。
（2）Selenium
①基于浏览器驱动的自动化测试工具，可以模拟用户在浏览器中的操作。
②支持 JavaScript 渲染，可以获取动态内容。
③可以实现复杂的交互操作，如点击按钮、填写表单等。
④需要安装浏览器驱动，增加了环境配置的复杂度。
⑤执行速度相对较慢，不适合大规模爬取。
（3）DrissionPage
①结合了 Requests 和 Selenium 的优点，是一个高效的爬虫框架，使用 Requests 获取页面源码，再交给 Selenium 处理 JavaScript 渲染和交互操作。
②兼具 Requests 和 Selenium 的优点，既能快速获取静态内容，又能处理动态内容和交互操作。

4.参考链接
https://www.bilibili.com/video/BV1Xa4y197Vw/?p=15&spm_id_from=pageDriver&vd_source=3b01b9f4ab4a2adbb8291da91fc030e7

5.遇到的主要问题以及可以优化的方向
（1）网站的反爬机制
使用无痕浏览器可以爬多一点，我爬取的网站大概可以爬取13页左右，这里我选择只爬取10页；每次要重新进入获取url和cookie信息，否则网站又会开启反爬机制
（2）采用多线程爬取：效率更高
（3）使用网站自动化爬取时不进行网页跳转

6.爬虫过程中获取的技巧
（1）正则表达式修改样式
（2）分割数据
（3）通过读取中间值检查代码


三、数据处理
1.将城市映射为省份：convert_to_province.py
(1)城市省份映射字典
(2)取城市对应的省份

2.将薪资格式统一为万元为单位：convert_to_salary.py
（1）检查数据并分析结构：'8千-1.2万', '2.5-3.5万/年', '15-25万/年', '6-8千','4.5-7千·13薪'
（2）“薪”确定年薪计算的月份
（3）用“-”对数据进行lower，upper分割并将对应的千和万去掉
（4）取最高和最低薪资的平均值返回数值

四、可视化呈现
此部分最初采用了maltplotlib库进行绘图、pandas进行数据处理，之后为了便于前端更换使用pyecharts库进行可视化呈现。
1.数据分析师关键词词云：Wordcloud.py
(1)使用jieba进行中文分词
(2)使用停用词列表去除部分无效词，并将单个词去除


2.地图显示不同地域工资均值：Map_salary.py
(1)使用pyecharts直接绘制地图
(2)使用Shapefile加载中国地图，匹配个省份的平均工资
(3)通过颜色深浅反应不同地区的工资水准


3.不同城市薪资排行：City_salary.py
(1)对不同城市的薪资计算平均值再排序


4.不同学历平均工资横向柱状图：Education_salary.py
(1)进行数据清洗，筛选出['高中', '大专', '本科']



5.不同规模公司数目：Grouped_number.py
(1)利用groupy函数计算不同规模公司数目
(2)将数目转化为百分比进行呈现


五、连接数据库
1.数据库选择：MongoDB
2.数据库安装
(1)参考链接：https://blog.csdn.net/chenghao1012/article/details/138333533
(2)安装中遇到的问题：电脑为中文名需要修改为英文名

(3)学到的安装知识
①Zip为安装文件材料，需要修改文件配置
②Msi就是我们常用的图形安装界面
③有两个安装包，大的那个是离线安装包，小的是离线安装包

3.数据库数据导入
(1)使用MongoDBcompass图形可视化工具
(2)启动MongoDB服务
(3)连接本地url：mongodb://localhost:27017
(4)创建collection直接导入数据

4.数据库连接
(1)使用pymongo库的MongoClient
(2)通过client和db获取数据库collection

5.将代码的数据源改为从数据库连接（一开始是使用爬虫导出的csv文件）
(1)创建collection直接导入数据库的数据
(2)将数据存储为DataFrame格式
(3)将数据可视化代码封装成函数
(4)函数一律返回图表

六、可视化平台呈现
1.使用pyecharts库的Page
(1)使用Page函数通过DraggablePageLayout生成各个可视化图像集合的html
(2)通过拖拽修改成自己想要的布局
(3)保存config.js文件
(4)根据js文件重新生成自己想要的html文件

2.使用pyecharts库的Table
(1)使用table生成带导航可筛选的html
(2)修改tab部分，加上标题
(3)修改css样式

3.遇到的未解决的问题以及可以提升的点
(1)单独将html保存无效——需要进一步加深对html各个部件的熟悉程度，以观测到底是哪个部分的链接出了问题
(2)可以尝试自己重新写一个html页面，加上登录注册之类的，包括后端数据库的衔接

七、项目展示（略）

3.功能详述
(1)城市薪资对比： 展示不同城市的平均薪资水平，帮助用户了解各个城市的就业竞争情况和薪资水平。
(2)学历工资对比： 分析不同学历对应的平均工资水平，为用户选择合适的学历和提升学历水平提供参考。
(3)公司规模招聘数量： 显示不同规模公司的招聘数量，帮助用户了解各个规模公司的招聘趋势。
(4)各省份平均工资： 展示各个省份的平均工资水平，为用户选择工作地点提供参考。
(5)岗位描述词云： 根据招聘信息中的岗位描述生成词云图，直观展示热门岗位的关键词，帮助用户了解当前热门岗位的特点和需求。

八、项目总结
1.总结
通过本项目，我成功实现了对 51job 网站招聘信息的爬取，并将其数据进行了可视化处理。通过数据可视化页面，用户可以直观地了解就业市场的情况和趋势，为其求职和就业提供了有力的参考依据。

2.个人反馈
通过这次项目经验，我已经基本掌握了requests、Selenium、DrissionPage三种爬虫方法，并且了解到了数据库连接以及编程方法，基本掌握mysql和Mongodb两类数据库的连接具体事项，对于数据分析，熟练使用maltplotlib库进行绘图，并且基本掌握了pyecharts的绘图以及html的page、table等多种页面生成方式。
但是在完成项目的过程中，我发现了一些之后可以修改提升的地方。
（1）爬虫可以加上多线程，提高爬虫的效率
（2）尝试加上一个完整的前后端框架，实现实时数据爬取分析
（3）细致修改数据可视化呈现，做成科技化的数据可视化大屏（这次书要是时间有限没有进行细致的样式修改，实际的样式修改操作并不复杂）
（4）增加数据可视化呈现多样化，可以增加折线图、散点图等
